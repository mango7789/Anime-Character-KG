import re
import torch
import pickle
import random
import ollama
from transformers import BertTokenizer
from py2neo import Graph

import ner_model as zwk  # ä½ å·²æœ‰çš„ ner_model.py


# ===============================
# 1. åŠ è½½æ¨¡å‹ & èµ„æº
# ===============================


def load_resources():
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    # NER ç›¸å…³
    with open("tmp_data/tag2idx.npy", "rb") as f:
        tag2idx = pickle.load(f)
    idx2tag = list(tag2idx)

    rule = zwk.rule_find()
    tfidf_r = zwk.tfidf_alignment()

    model_name = "model/chinese-roberta-wwm-ext"
    tokenizer = BertTokenizer.from_pretrained(model_name)

    ner_model = zwk.Bert_Model(
        model_name=model_name, hidden_size=128, tag_num=len(tag2idx), bi=True
    )
    ner_model.load_state_dict(
        torch.load("model/best_roberta_rnn_model_ent_aug.pt", map_location=device)
    )
    ner_model.to(device)
    ner_model.eval()

    # Neo4j
    graph = Graph("http://localhost:7474", user="neo4j", password="your_password")

    return ner_model, tokenizer, rule, tfidf_r, idx2tag, device, graph


# ===============================
# 2. æ„å›¾è¯†åˆ«ï¼ˆåŠ¨æ¼«ç‰ˆï¼‰
# ===============================


def Intent_Recognition(query, llm_name="qwen:32b"):
    prompt = f"""
ä½ éœ€è¦åˆ¤æ–­ç”¨æˆ·åœ¨åŠ¨æ¼«è§’è‰²çŸ¥è¯†å›¾è°±ä¸­çš„æŸ¥è¯¢æ„å›¾ã€‚

ã€æŸ¥è¯¢ç±»åˆ«ã€‘
- æŸ¥è¯¢è§’è‰²å£°ä¼˜
- æŸ¥è¯¢è§’è‰²æ‰€å±ä½œå“
- æŸ¥è¯¢è§’è‰²å…³ç³»

ã€ç¤ºä¾‹ã€‘
è¾“å…¥ï¼šè·¯é£çš„å£°ä¼˜æ˜¯è°ï¼Ÿ
è¾“å‡ºï¼š["æŸ¥è¯¢è§’è‰²å£°ä¼˜"] # è¯¢é—®è§’è‰²çš„é…éŸ³æ¼”å‘˜

è¾“å…¥ï¼šé¸£äººæ˜¯å“ªä¸ªä½œå“é‡Œçš„ï¼Ÿ
è¾“å‡ºï¼š["æŸ¥è¯¢è§’è‰²æ‰€å±ä½œå“"] # æŸ¥è¯¢è§’è‰²æ¥æºä½œå“

è¾“å…¥ï¼šä½åŠ©å’Œé¸£äººæ˜¯ä»€ä¹ˆå…³ç³»ï¼Ÿ
è¾“å‡ºï¼š["æŸ¥è¯¢è§’è‰²å…³ç³»"] # æŸ¥è¯¢è§’è‰²ä¹‹é—´çš„å…³ç³»

ã€è¦æ±‚ã€‘
- è¾“å‡ºå¿…é¡»æ¥è‡ªä¸Šè¿°æŸ¥è¯¢ç±»åˆ«
- ä¸è¶…è¿‡ 2 ä¸ª
- è¾“å‡ºåç”¨ # ç®€è¦è§£é‡Š

é—®é¢˜è¾“å…¥ï¼š"{query}"
"""

    resp = ollama.generate(model=llm_name, prompt=prompt)["response"]
    return resp


# ===============================
# 3. KG æŸ¥è¯¢ + Prompt æ„é€ 
# ===============================


def build_prompt(intent_result, query, entities, graph):
    prompt = "<æŒ‡ä»¤>ä½ æ˜¯ä¸€ä¸ªåŠ¨æ¼«è§’è‰²çŸ¥è¯†é—®ç­”åŠ©æ‰‹ï¼Œå¿…é¡»å®Œå…¨åŸºäºæç¤ºå›ç­”ã€‚</æŒ‡ä»¤>"
    prompt += "<æŒ‡ä»¤>å¦‚æœæç¤ºä¸­æ²¡æœ‰ç­”æ¡ˆï¼Œå¿…é¡»å›ç­”â€œæ ¹æ®å·²çŸ¥ä¿¡æ¯æ— æ³•å›ç­”è¯¥é—®é¢˜â€ã€‚</æŒ‡ä»¤>"

    used = False

    # ---------- æŸ¥è¯¢å£°ä¼˜ ----------
    if "å£°ä¼˜" in intent_result and "è§’è‰²" in entities:
        role = entities["è§’è‰²"]
        cypher = f"""
        MATCH (a:è§’è‰² {{åç§°:'{role}'}})-[:é…éŸ³]->(b:å£°ä¼˜)
        RETURN b.åç§°
        """
        res = graph.run(cypher).data()
        prompt += "<æç¤º>"
        prompt += f"ç”¨æˆ·æŸ¥è¯¢è§’è‰² {role} çš„å£°ä¼˜ï¼ŒçŸ¥è¯†å›¾è°±ä¿¡æ¯å¦‚ä¸‹ï¼š"
        if res:
            prompt += "ã€".join([list(r.values())[0] for r in res])
        else:
            prompt += "å›¾è°±ä¸­æ— ç›¸å…³ä¿¡æ¯ã€‚"
        prompt += "</æç¤º>"
        used = True

    # ---------- æŸ¥è¯¢ä½œå“ ----------
    if "ä½œå“" in intent_result and "è§’è‰²" in entities:
        role = entities["è§’è‰²"]
        cypher = f"""
        MATCH (a:è§’è‰² {{åç§°:'{role}'}})-[:ç™»åœºäº]->(b:ä½œå“)
        RETURN b.åç§°
        """
        res = graph.run(cypher).data()
        prompt += "<æç¤º>"
        prompt += f"ç”¨æˆ·æŸ¥è¯¢è§’è‰² {role} æ‰€å±ä½œå“ï¼ŒçŸ¥è¯†å›¾è°±ä¿¡æ¯å¦‚ä¸‹ï¼š"
        if res:
            prompt += "ã€".join([list(r.values())[0] for r in res])
        else:
            prompt += "å›¾è°±ä¸­æ— ç›¸å…³ä¿¡æ¯ã€‚"
        prompt += "</æç¤º>"
        used = True

    # ---------- æŸ¥è¯¢è§’è‰²å…³ç³» ----------
    if "å…³ç³»" in intent_result and "è§’è‰²" in entities:
        role = entities["è§’è‰²"]
        cypher = f"""
        MATCH (a:è§’è‰² {{åç§°:'{role}'}})-[r]->(b:è§’è‰²)
        RETURN type(r) AS rel, b.åç§° AS target
        """
        res = graph.run(cypher).data()
        prompt += "<æç¤º>"
        prompt += f"ç”¨æˆ·æŸ¥è¯¢è§’è‰² {role} çš„è§’è‰²å…³ç³»ï¼ŒçŸ¥è¯†å›¾è°±ä¿¡æ¯å¦‚ä¸‹ï¼š"
        if res:
            rels = [f"{r['rel']} â†’ {r['target']}" for r in res]
            prompt += "ï¼›".join(rels)
        else:
            prompt += "å›¾è°±ä¸­æ— ç›¸å…³ä¿¡æ¯ã€‚"
        prompt += "</æç¤º>"
        used = True

    if not used:
        prompt += "<æç¤º>çŸ¥è¯†åº“ä¸­æ²¡æœ‰å¯ç”¨ä¿¡æ¯ã€‚</æç¤º>"

    prompt += f"<ç”¨æˆ·é—®é¢˜>{query}</ç”¨æˆ·é—®é¢˜>"
    return prompt


# ===============================
# 4. ä¸»æµç¨‹
# ===============================


def main():
    ner_model, tokenizer, rule, tfidf_r, idx2tag, device, graph = load_resources()
    llm_name = "qwen:32b"

    print("ğŸŒ åŠ¨æ¼«è§’è‰²çŸ¥è¯†é—®ç­”ç³»ç»Ÿå·²å¯åŠ¨ï¼ˆè¾“å…¥ exit é€€å‡ºï¼‰")

    while True:
        query = input("\nç”¨æˆ·ï¼š")
        if query.lower() in ["exit", "quit"]:
            break

        # â‘  å®ä½“è¯†åˆ«
        entities = zwk.get_ner_result(
            ner_model, tokenizer, query, rule, tfidf_r, device, idx2tag
        )
        print(f"[NER] {entities}")

        # â‘¡ æ„å›¾è¯†åˆ«
        intent = Intent_Recognition(query, llm_name)
        print(f"[Intent] {intent}")

        # â‘¢ æ„é€  Prompt + æŸ¥ KG
        prompt = build_prompt(intent, query, entities, graph)

        # â‘£ LLM è¾“å‡ºç­”æ¡ˆ
        answer = ollama.chat(
            model=llm_name, messages=[{"role": "user", "content": prompt}]
        )["message"]["content"]

        print(f"\nåŠ©æ‰‹ï¼š{answer}")


if __name__ == "__main__":
    main()
