'''
import os
import ahocorasick
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
'''

# ===============================
# 1. è§„åˆ™å®ä½“åŒ¹é…ï¼ˆAho-Corasickï¼‰
# ===============================

'''
class RuleNER:
    """
    åªè´Ÿè´£è¯†åˆ«ã€å¯æŒ‡ç§°å®ä½“èŠ‚ç‚¹ã€‘
    """

    def __init__(self, ent_dir="data/ent_aug"):
        # âœ… åªä¿ç•™ entity_types
        self.entity_types = [
            "Work",
            "Character",
            "Person",
            "Organization",
            "Group",
            "Location",
        ]

        self.type2idx = {t: i for i, t in enumerate(self.entity_types)}
        self.automata = [ahocorasick.Automaton() for _ in self.entity_types]

        for ent_type in self.entity_types:
            path = os.path.join(ent_dir, f"{ent_type}.txt")
            if not os.path.exists(path):
                continue

            with open(path, encoding="utf-8") as f:
                for line in f:
                    ent = line.strip()
                    if ent:
                        self.automata[self.type2idx[ent_type]].add_word(ent, ent)

        for a in self.automata:
            a.make_automaton()
        

    def find(self, text):

        results = []
        used = set()

        for idx, automaton in enumerate(self.automata):
            etype = self.entity_types[idx]
            for end, word in automaton.iter(text):
                start = end - len(word) + 1
                if any(i in used for i in range(start, end + 1)):
                    continue
                results.append((start, end, etype, word))
                for i in range(start, end + 1):
                    used.add(i)

        return results
'''
'''
class RuleNER:
    """
    åªè´Ÿè´£è¯†åˆ«ã€å¯æŒ‡ç§°å®ä½“èŠ‚ç‚¹ã€‘
    """

    def __init__(self, ent_dir="data/ent_aug", min_len=2):
        self.entity_types = [
            "Work",
            "Character",
            "Person",
            "Organization",
            "Group",
            "Location",
        ]

        self.type2idx = {t: i for i, t in enumerate(self.entity_types)}
        self.automata = [ahocorasick.Automaton() for _ in self.entity_types]

        for ent_type in self.entity_types:
            path = os.path.join(ent_dir, f"{ent_type}.txt")
            if not os.path.exists(path):
                continue

            idx = self.type2idx[ent_type]

            with open(path, encoding="utf-8") as f:
                for line in f:
                    ent = line.strip()
                    if not ent:
                        continue

                    # 1ï¸âƒ£ å®Œæ•´å®ä½“
                    self.automata[idx].add_word(ent, (ent, ent))

                    # 2ï¸âƒ£ ç‚¹åˆ†å‰²å
                    if "Â·" in ent:
                        key = ent.split("Â·")[-1]
                        if len(key) >= min_len:
                            self.automata[idx].add_word(key, (key, ent))

                    # 3ï¸âƒ£ ä¸­æ–‡å§“åå°¾éƒ¨
                    if ent_type in ("Character", "Person") and len(ent) >= 3:
                        key = ent[-2:]
                        if len(key) >= min_len:
                            self.automata[idx].add_word(key, (key, ent))


        for a in self.automata:
            a.make_automaton()


    def find(self, text):
        """
        è¿”å›ï¼š
        [(start, end, entity_type, canonical_entity), ...]
        """
        results = []
        used = set()

        for idx, automaton in enumerate(self.automata):
            etype = self.entity_types[idx]
            for end, (alias, canonical) in automaton.iter(text):
                start = end - len(alias) + 1

                if any(i in used for i in range(start, end + 1)):
                    continue

                results.append((start, end, etype, canonical))
                for i in range(start, end + 1):
                    used.add(i)

        return results


# ===============================
# 2. TF-IDF å®ä½“è§„èŒƒåŒ–
# ===============================


class TFIDFAligner:
    """
    ç”¨äºï¼š
    - å¤„ç†åˆ«å
    - æ¨¡ç³ŠåŒ¹é…
    """

    def __init__(self, ent_dir="data/ent_aug"):
        self.type2ents = {}
        self.type2vecs = {}
        self.type2tfidf = {}

        for file in os.listdir(ent_dir):
            if not file.endswith(".txt"):
                continue

            ent_type = file.replace(".txt", "")
            path = os.path.join(ent_dir, file)

            with open(path, encoding="utf-8") as f:
                ents = [l.strip() for l in f if l.strip()]

            if not ents:
                continue

            tfidf = TfidfVectorizer(analyzer="char")
            vecs = tfidf.fit_transform(ents).toarray()

            self.type2ents[ent_type] = ents
            self.type2vecs[ent_type] = vecs
            self.type2tfidf[ent_type] = tfidf

    def align(self, ner_results, threshold=0.5):
        """
        è¾“å‡ºæ ¼å¼ï¼š
        {
            "Work": ["å’’æœ¯å›æˆ˜"],
            "Character": ["äº”æ¡æ‚Ÿ"]
        }
        """
        result = {}

        for _, _, ent_type, word in ner_results:
            if ent_type not in self.type2tfidf:
                continue

            qv = self.type2tfidf[ent_type].transform([word])
            sims = cosine_similarity(qv, self.type2vecs[ent_type])[0]
            idx = sims.argmax()

            if sims[idx] >= threshold:
                result.setdefault(ent_type, []).append(self.type2ents[ent_type][idx])

        return result


# ===============================
# 3. å¯¹å¤–ç»Ÿä¸€æ¥å£
# ===============================


def get_ner_result(model, tokenizer, text, rule_ner, tfidf_aligner, device, idx2tag):
    """
    âš ï¸ model / tokenizer / device / idx2tag ä¿ç•™
    åªæ˜¯ä¸ºäº†æ¥å£å…¼å®¹ anime_kgqa.py
    """

    ner_raw = rule_ner.find(text)
    entities = tfidf_aligner.align(ner_raw)

    return entities
'''
import os
import ahocorasick
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity


# ===============================
# 1. è§„åˆ™å®ä½“åŒ¹é…ï¼ˆAho-Corasickï¼‰
# ===============================

class RuleNER:
    """
    åªè´Ÿè´£è¯†åˆ«ã€å¯æŒ‡ç§°å®ä½“èŠ‚ç‚¹ã€‘
    """

    def __init__(self, ent_dir="data/ent_aug", min_len=2):
        self.entity_types = [
            "Work",
            "Character",
            "Person",
            "Organization",
            "Group",
            "Location",
        ]

        self.type2idx = {t: i for i, t in enumerate(self.entity_types)}
        self.automata = [ahocorasick.Automaton() for _ in self.entity_types]

        for ent_type in self.entity_types:
            path = os.path.join(ent_dir, f"{ent_type}.txt")
            if not os.path.exists(path):
                continue

            idx = self.type2idx[ent_type]

            with open(path, encoding="utf-8") as f:
                for line in f:
                    ent = line.strip()
                    if not ent:
                        continue

                    # 1ï¸âƒ£ å®Œæ•´å®ä½“
                    self.automata[idx].add_word(ent, (ent, ent))

                    # 2ï¸âƒ£ ç‚¹åˆ†å‰²åï¼šè’™å¥‡Â·DÂ·è·¯é£ â†’ è·¯é£
                    if "Â·" in ent:
                        key = ent.split("Â·")[-1]
                        if len(key) >= min_len:
                            self.automata[idx].add_word(key, (key, ent))

                    # 3ï¸âƒ£ ä¸­æ–‡å§“åå°¾éƒ¨ï¼šæ¼©æ¶¡é¸£äºº â†’ é¸£äºº
                    if ent_type in ("Character", "Person") and len(ent) >= 3:
                        key = ent[-2:]
                        if len(key) >= min_len:
                            self.automata[idx].add_word(key, (key, ent))

        for a in self.automata:
            a.make_automaton()

    def find(self, text):
        """
        è¿”å›ï¼š
        [(start, end, entity_type, canonical_entity), ...]
        """
        results = []
        used = set()

        for idx, automaton in enumerate(self.automata):
            etype = self.entity_types[idx]
            for end, (alias, canonical) in automaton.iter(text):
                start = end - len(alias) + 1

                if any(i in used for i in range(start, end + 1)):
                    continue

                results.append((start, end, etype, canonical))
                for i in range(start, end + 1):
                    used.add(i)

        return results
# ===============================
# 2. TF-IDF å®ä½“è§„èŒƒåŒ– / å…œåº•
# ===============================

class TFIDFAligner:
    """
    ç”¨äºï¼š
    - åˆ«åå¯¹é½
    - æ¨¡ç³ŠåŒ¹é…
    - å…¨é‡å…œåº•æœç´¢
    """

    def __init__(self, ent_dir="data/ent_aug"):
        self.type2ents = {}
        self.type2vecs = {}
        self.type2tfidf = {}

        for file in os.listdir(ent_dir):
            if not file.endswith(".txt"):
                continue

            ent_type = file.replace(".txt", "")
            path = os.path.join(ent_dir, file)

            with open(path, encoding="utf-8") as f:
                ents = [l.strip() for l in f if l.strip()]

            if not ents:
                continue

            tfidf = TfidfVectorizer(analyzer="char")
            vecs = tfidf.fit_transform(ents).toarray()

            self.type2ents[ent_type] = ents
            self.type2vecs[ent_type] = vecs
            self.type2tfidf[ent_type] = tfidf

    def align(self, ner_results, threshold=0.5):
        """
        å¯¹è§„åˆ™ NER çš„ç»“æœåšè§„èŒƒåŒ–
        """
        result = {}

        for _, _, ent_type, word in ner_results:
            if ent_type not in self.type2tfidf:
                continue

            qv = self.type2tfidf[ent_type].transform([word])
            sims = cosine_similarity(qv, self.type2vecs[ent_type])[0]
            idx = sims.argmax()

            if sims[idx] >= threshold:
                result.setdefault(ent_type, []).append(
                    self.type2ents[ent_type][idx]
                )

        return result

    def search_best(self, query, ent_type="Character", threshold=0.3):
        """
        ğŸ”¥ å½“ NER å®Œå…¨å¤±è´¥æ—¶çš„å…œåº•ï¼š
        åœ¨æŒ‡å®šå®ä½“ç±»å‹å…¨é›†ä¸­æ‰¾æœ€ç›¸ä¼¼å®ä½“
        """
        if ent_type not in self.type2tfidf:
            return None

        tfidf = self.type2tfidf[ent_type]
        ents = self.type2ents[ent_type]
        vecs = self.type2vecs[ent_type]

        qv = tfidf.transform([query])
        sims = cosine_similarity(qv, vecs)[0]

        idx = sims.argmax()
        score = sims[idx]

        if score >= threshold:
            return ents[idx]

        return None
# ===============================
# 3. å¯¹å¤–ç»Ÿä¸€æ¥å£ï¼ˆå…³é”®ï¼‰
# ===============================

def get_ner_result(model, tokenizer, text, rule_ner, tfidf_aligner, device, idx2tag):
    """
    ä¼˜å…ˆè§„åˆ™ NER
    è§„åˆ™ NER å¤±è´¥ â†’ TF-IDF å…¨é‡å…œåº•
    """

    # â‘  è§„åˆ™ NER
    ner_raw = rule_ner.find(text)

    if ner_raw:
        return tfidf_aligner.align(ner_raw)

    # â‘¡ å…œåº•ï¼šCharacter å®ä½“å…¨é›†æ¨¡ç³Šæœç´¢
    fallback = tfidf_aligner.search_best(
        query=text,
        ent_type="Character",
        threshold=0.1,
    )

    if fallback:
        return {"Character": [fallback]}

    return {}
